import os
import re
import difflib
import json
import pandas as pd
import gradio as gr
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.text_splitter import TokenTextSplitter
from langchain.agents import initialize_agent, AgentType
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.document_loaders import PyPDFLoader

# ìœ í‹¸ í•¨ìˆ˜
def normalize(text):
    text = re.sub(r"\(.*?\)", "", text)
    text = re.sub(r"[^\wê°€-í£]", "", text)
    return re.sub(r"\s+", "", text.strip().lower())

def clean_product_name(query):
    query = re.sub(r"(ì— ëŒ€í•´.*|ì•Œë ¤ì¤˜|ë¬´ì—‡ì…ë‹ˆê¹Œ|ë­”ê°€ìš”|ë­ì•¼|ì–´ë–»ê²Œ.*|ì‚¬ìš©ë²•|ë¶€ì‘ìš©|íš¨ëŠ¥|ë³µìš©.*|ì„­ì·¨.*|íˆ¬ì—¬.*)", "", query)
    query = re.sub(r"[^\wê°€-í£]", "", query)
    query = re.sub(r"(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼)$", "", query)
    return query.strip()

def is_product_match(query, target_name):
    nq, nt = normalize(query), normalize(target_name)
    return nq in nt or nt in nq or difflib.SequenceMatcher(None, nq, nt).ratio() > 0.85

def extract_field_from_docs(docs, label):
    pattern = rf"\[{label}\]:\s*((?:.|\n)*?)(?=\n\[|\Z)"
    candidates = []
    for doc in docs:
        match = re.search(pattern, doc.page_content)
        if match:
            candidates.append(match.group(1).strip())
    if candidates:
        return max(candidates, key=len)
    return "ì •ë³´ ì—†ìŒ"

# LLM ì´ˆê¸°í™”
load_dotenv(".env")
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["TAVILY_API_KEY"] = "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"
llm = ChatOpenAI(model="gpt-4o", temperature=0)
hf_model = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
compressor = CrossEncoderReranker(model=hf_model, top_n=5)
splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=50)

condition_risk_map = {
    "ìœ„ì¥ì—¼": ["ìœ„ì¥ ìê·¹", "ì†ì“°ë¦¼", "êµ¬í† ", "ì†Œí™”ë¶ˆëŸ‰"],
    "ê°„ì§ˆí™˜": ["ê°„ë…ì„±", "ê°„ ì†ìƒ"],
    "ê³ í˜ˆì••": ["í˜ˆì•• ìƒìŠ¹", "ì‹¬ì¥ ë¶€ë‹´"]
}

def extract_condition_and_category(query):
    prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë³‘ë ¥ê³¼ ì•½ë¬¼ ì¢…ë¥˜ë¥¼ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ìì—°ì–´ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ:
{{
  "ë³‘ë ¥": "ìœ„ì¥ì—¼",
  "ì•½ë¬¼ì¢…ë¥˜": "ê°ê¸°ì•½"
}}

ì§ˆë¬¸: "{query}"

ë°˜í™˜:
"""
    try:
        response = llm.invoke(prompt)
        raw_content = response.content.strip()
        print("ğŸ“¤ LLM ì‘ë‹µ ë‚´ìš© í™•ì¸:", raw_content)

        # âœ… LLMì´ ```json ... ``` ìœ¼ë¡œ ê°ì‹¸ì„œ ë³´ëƒˆì„ ê²½ìš° ì œê±°
        if raw_content.startswith("```"):
            raw_content = re.sub(r"```[a-zA-Z]*", "", raw_content).strip()  # ```json ì œê±°
            raw_content = raw_content.replace("```", "").strip()            # ë§ˆì§€ë§‰ ``` ì œê±°

        return json.loads(raw_content)  # ì´ì œ íŒŒì‹± ì„±ê³µ!
    except Exception as e:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        return {"ë³‘ë ¥": "", "ì•½ë¬¼ì¢…ë¥˜": ""}

def generate_recommendation_response(condition, category, candidates):
    prompt = f"""
ë‹¹ì‹ ì€ ê±´ê°• ìƒíƒœì— ë§ëŠ” ì•½ì„ ì¶”ì²œí•´ì£¼ëŠ” ê±´ê°• ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì‚¬ìš©ìëŠ” í˜„ì¬ '{condition}'ì´ ìˆê³ , '{category}'ì— ë„ì›€ì´ ë˜ëŠ” ì•½ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ëŠ” ë³‘ë ¥ì— ë”°ë¼ ë¶€ì‘ìš© ìœ„í—˜ì´ ë‚®ì€ ì¶”ì²œ í›„ë³´ë“¤ì…ë‹ˆë‹¤. ê° ì•½ì— ëŒ€í•´ ë¶€ë“œëŸ½ê³  ì‹ ë¢°ê° ìˆê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ì¶”ì²œ ì´ìœ ë„ ë³‘ë ¥ê³¼ ê´€ë ¨ì§€ì–´ ì£¼ì„¸ìš”.

ì¶”ì²œ í›„ë³´:
{json.dumps(candidates, ensure_ascii=False)}

ë‹µë³€:
"""
    return llm.invoke(prompt).content

# í•„ë“œ ê°ì§€ (LLM ê¸°ë°˜)
def detect_requested_fields_llm(query):
    prompt = f"""
ì•„ë˜ì˜ ì§ˆë¬¸ì—ì„œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” í•­ëª©ì´ ë¬´ì—‡ì¸ì§€ ì¶”ë¡ í•˜ì„¸ìš”.
í•­ëª©ì€ [\"íš¨ëŠ¥\", \"ë¶€ì‘ìš©\", \"ì‚¬ìš©ë²•\"] ì¤‘ í•´ë‹¹ë˜ëŠ” ê²ƒë§Œ í¬í•¨í•˜ì„¸ìš”.
ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì£¼ì„¸ìš”.

ì§ˆë¬¸: \"{query}\"
ê²°ê³¼:
"""
    try:
        response = llm.invoke(prompt)
        return json.loads(response.content.strip())
    except:
        return ["íš¨ëŠ¥", "ë¶€ì‘ìš©", "ì‚¬ìš©ë²•"]

# ë‹µë³€ ìƒì„± (LLM í‘œí˜„)
def generate_response_llm(name, fields, eff, side, usage):
    context = {"ì œí’ˆëª…": name, "íš¨ëŠ¥": eff, "ë¶€ì‘ìš©": side, "ì‚¬ìš©ë²•": usage}
    field_info = {k: v for k, v in context.items() if k in fields and v not in ["ì •ë³´ ì—†ìŒ", ""]}
    prompt = f"""
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì •í™•í•œ ê±´ê°• ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì•„ë˜ì— ì£¼ì–´ì§„ ì•½í’ˆ ì •ë³´(JSON í˜•ì‹)ë¥¼ ì‚¬ìš©ìì—ê²Œ ë¶€ë“œëŸ½ê³  ì‹ ë¢°ê° ìˆê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³  ì¶”ê°€ì ì¸ ì¶”ë¡ ì€ í•˜ì§€ ë§ˆì„¸ìš”.

ìš”ì²­ í•­ëª©: {fields}
ì •ë³´:
{json.dumps(field_info, ensure_ascii=False)}

ë‹µë³€:
"""
    return llm.invoke(prompt).content

# ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (LLM JSON êµ¬ì¡°)
def summarize_structured_json(text):
    prompt = f"""
ë‹¤ìŒ ì•½í’ˆ ê´€ë ¨ í…ìŠ¤íŠ¸ì—ì„œ í•­ëª©ë³„ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
í•­ëª©ì€ 'ì œí’ˆëª…', 'íš¨ëŠ¥', 'ë¶€ì‘ìš©', 'ì‚¬ìš©ë²•'ì´ë©°, ì—†ìœ¼ë©´ \"ì •ë³´ ì—†ìŒ\"ìœ¼ë¡œ í‘œê¸°í•´ì¤˜.

í…ìŠ¤íŠ¸:
{text}

ê²°ê³¼ í˜•ì‹:
{{
  "ì œí’ˆëª…": "...",
  "íš¨ëŠ¥": "...",
  "ë¶€ì‘ìš©": "...",
  "ì‚¬ìš©ë²•": "..."
}}
"""
    try:
        response = llm.invoke(prompt)
        return json.loads(response.content)
    except:
        return {"ì œí’ˆëª…": "", "íš¨ëŠ¥": "ì •ë³´ ì—†ìŒ", "ë¶€ì‘ìš©": "ì •ë³´ ì—†ìŒ", "ì‚¬ìš©ë²•": "ì •ë³´ ì—†ìŒ"}

# PDF ì¸ë±ì‹±
pdf_path = "C:\\Users\\jung\\Desktop\\pdf\\í•œêµ­ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ì¼ë°˜ì˜ì•½í’ˆ 20ì„ .pdf"
pdf_docs_raw = PyPDFLoader(pdf_path).load()
pdf_structured_docs, pdf_product_index = [], {}
for doc in pdf_docs_raw:
    blocks = re.findall(r"(\d+\.\s*.+?)(?=\n\d+\.|\Z)", doc.page_content, re.DOTALL)
    for block in blocks:
        name_match = re.match(r"\d+\.\s*([^\n(]+)", block)
        if name_match:
            name = name_match.group(1).strip()
            eff = re.search(r"ì£¼ìš” íš¨ëŠ¥[:ï¼š]\s*(.*?)(?:\n|ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š])", block, re.DOTALL)
            side = re.search(r"ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š]\s*(.*?)(?:\n|ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š])", block, re.DOTALL)
            usage = re.search(r"ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š]\s*(.*?)(?:\n|$)", block, re.DOTALL)
            content = f"[ì œí’ˆëª…]: {name}\n[íš¨ëŠ¥]: {eff.group(1).strip() if eff else 'ì •ë³´ ì—†ìŒ'}\n[ë¶€ì‘ìš©]: {side.group(1).strip() if side else 'ì •ë³´ ì—†ìŒ'}\n[ì‚¬ìš©ë²•]: {usage.group(1).strip() if usage else 'ì •ë³´ ì—†ìŒ'}"
            for chunk in splitter.split_text(content):
                doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name})
                pdf_structured_docs.append(doc_obj)
                pdf_product_index.setdefault(normalize(name), []).append(doc_obj)
pdf_vectordb = FAISS.from_documents(pdf_structured_docs, OpenAIEmbeddings())
pdf_retriever = ContextualCompressionRetriever(
    base_retriever=pdf_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# Excel ì¸ë±ì‹±
excel_files = [rf"C:\\Users\\jung\\Desktop\\11\\eì•½ì€ìš”ì •ë³´ê²€ìƒ‰{i}.xlsx" for i in range(1, 11)]
required_columns = ["ì œí’ˆëª…", "ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?", "ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?", "ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?"]
doc_list, product_names, product_names_normalized = [], [], []
for file in excel_files:
    if not os.path.exists(file): continue
    df = pd.read_excel(file)
    if not all(col in df.columns for col in required_columns): continue
    df = df[required_columns].fillna("ì •ë³´ ì—†ìŒ")
    for _, row in df.iterrows():
        name = row["ì œí’ˆëª…"].strip()
        product_names.append(name)
        product_names_normalized.append(normalize(name))
        content = f"[ì œí’ˆëª…]: {name}\n[íš¨ëŠ¥]: {row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']}\n[ë¶€ì‘ìš©]: {row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']}\n[ì‚¬ìš©ë²•]: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']}"
        for chunk in splitter.split_text(content):
            doc_list.append(Document(page_content=f"[ì œí’ˆëª…]: {name}\n{chunk}", metadata={"ì œí’ˆëª…": name}))
vectordb = FAISS.from_documents(doc_list, OpenAIEmbeddings())
excel_retriever = ContextualCompressionRetriever(
    base_retriever=vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# ì™¸ë¶€ ê²€ìƒ‰ ë° í‰ê°€
search_agent = initialize_agent(tools=[TavilySearchResults()], llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, handle_parsing_errors=True, verbose=False)
eval_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["answer", "query"],
        template="ì§ˆë¬¸: {query}\në‹µë³€: {answer}\n\nì‹ ë¢°ë„ í‰ê°€:\n- ì‹ ë¢°ë„: [ë†’ìŒ / ì¤‘ê°„ / ë‚®ìŒ]\n- ì´ìœ :"
    )
)

ragas_records = []

def _log_answer(query, docs, fields, override_name=None):
    name = override_name or extract_field_from_docs(docs, "ì œí’ˆëª…") or clean_product_name(query)
    answer = generate_response_llm(
        name, fields,
        extract_field_from_docs(docs, "íš¨ëŠ¥"),
        extract_field_from_docs(docs, "ë¶€ì‘ìš©"),
        extract_field_from_docs(docs, "ì‚¬ìš©ë²•")
    )
    ragas_records.append({
        "query": query,
        "contexts": [doc.page_content for doc in docs],
        "answer": answer
    })
    return answer

def qa_agent(query):
    original_query = query
    condition_data = extract_condition_and_category(query)
    print("ğŸ§ª ë³‘ë ¥ ì¶”ì¶œ ê²°ê³¼:", condition_data)
    condition = condition_data.get("ë³‘ë ¥", "").strip()
    category = condition_data.get("ì•½ë¬¼ì¢…ë¥˜", "").strip()

    if condition and category:
        risk_keywords = condition_risk_map.get(condition, [])
        keyword_candidates = []

        for doc in pdf_structured_docs:
            name = doc.metadata.get("ì œí’ˆëª…", "")
            content = doc.page_content
            if category in content or any(k in content for k in ["ê°ê¸°", "ê¸°ì¹¨", "ì½§ë¬¼", "í•´ì—´"]):
                if not any(risk in content for risk in risk_keywords):
                    rec = {
                        "ì œí’ˆëª…": name,
                        "íš¨ëŠ¥": extract_field_from_docs([doc], "íš¨ëŠ¥"),
                        "ë¶€ì‘ìš©": extract_field_from_docs([doc], "ë¶€ì‘ìš©"),
                        "ì‚¬ìš©ë²•": extract_field_from_docs([doc], "ì‚¬ìš©ë²•")
                    }
                    keyword_candidates.append(rec)

                    print("ğŸ” ì¶”ì²œ í›„ë³´ ì•½ë¬¼ ê°œìˆ˜:", len(keyword_candidates))  # ì—¬ê¸°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”
                    print("ğŸ“‹ ì¶”ì²œ í›„ë³´ ì˜ˆì‹œ:", keyword_candidates[:2])  # ì¼ë¶€ ì˜ˆì‹œ ë³´ê¸°

        if keyword_candidates:
            return generate_recommendation_response(condition, category, keyword_candidates[:3])
    stripped = clean_product_name(query)
    normalized_query = normalize(stripped)
    requested_fields = detect_requested_fields_llm(original_query)

    matches = pdf_product_index.get(normalized_query)
    if not matches:
        suggestion = difflib.get_close_matches(normalized_query, list(pdf_product_index.keys()), n=1, cutoff=0.85)
        if suggestion:
            matches = pdf_product_index[suggestion[0]]
    if matches:
        return _log_answer(query, matches, requested_fields)

    pdf_results = pdf_retriever.get_relevant_documents(stripped)
    matches = [doc for doc in pdf_results if is_product_match(stripped, doc.metadata.get("ì œí’ˆëª…", ""))]
    if matches:
        return _log_answer(query, matches, requested_fields)

    exact_product = None
    if normalized_query in product_names_normalized:
        idx = product_names_normalized.index(normalized_query)
        exact_product = product_names[idx]
    else:
        suggestions = difflib.get_close_matches(normalized_query, product_names_normalized, n=1, cutoff=0.85)
        if suggestions:
            idx = product_names_normalized.index(suggestions[0])
            exact_product = product_names[idx]

    if exact_product:
        results = excel_retriever.get_relevant_documents(exact_product)
        filtered = [doc for doc in results if normalize(doc.metadata.get("ì œí’ˆëª…", "")) == normalized_query]
        return _log_answer(query, filtered or results, requested_fields, override_name=exact_product)

    search_query = f"site:mfds.go.kr OR site:health.kr {stripped}"
    raw = search_agent.run(search_query)

    with ThreadPoolExecutor() as executor:
        summary_data = executor.submit(summarize_structured_json, raw).result()
        evaluation = executor.submit(eval_chain.invoke, {"answer": json.dumps(summary_data, ensure_ascii=False), "query": query}).result()

    answer = generate_response_llm(
        summary_data["ì œí’ˆëª…"] or stripped,
        requested_fields,
        summary_data["íš¨ëŠ¥"],
        summary_data["ë¶€ì‘ìš©"],
        summary_data["ì‚¬ìš©ë²•"]
    ) + f"\n\nğŸ”¬ í‰ê°€\n{evaluation['text']}"

    ragas_records.append({
        "query": original_query,
        "contexts": [raw],
        "answer": answer
    })
    return answer


demo = gr.Interface(
    fn=qa_agent,
    inputs=gr.Textbox(lines=2, placeholder="ì˜ˆ: ìŠ¬ë¦¼ë½ì •ì˜ ì‚¬ìš©ë²•ì€?"),
    outputs="text",
    title="ğŸ’Š ì˜ì•½í’ˆ ì •ë³´ ìƒë‹´ ì—ì´ì „íŠ¸ (LLM ê¸°ë°˜ RAG with í‰ê°€)",
    description="PDF â†’ Excel â†’ ì™¸ë¶€ ê²€ìƒ‰ ìˆœì„œë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
)
demo.launch()

with open("ragas_records.json", "w", encoding="utf-8") as f:
    json.dump(ragas_records, f, ensure_ascii=False, indent=2)
